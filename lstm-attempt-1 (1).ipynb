{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport numpy as np\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Input, LSTM, Embedding, Dense\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.layers import Lambda","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install tensorflow_addons","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import tensorflow_addons as tfa","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.listdir('../input/abstraction-and-reasoning-challenge')","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"['test', 'evaluation', 'sample_submission.csv', 'training']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n\ndata_path = Path('/kaggle/input/abstraction-and-reasoning-challenge/')\ntraining_path = data_path / 'training'\nevaluation_path = data_path / 'evaluation'\ntest_path = data_path / 'test'","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = os.listdir(training_path)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task_file = str(training_path / l[1])\n\nwith open(task_file, 'r') as f:\n    task = json.load(f)\n\nprint(task.keys())","execution_count":7,"outputs":[{"output_type":"stream","text":"dict_keys(['train', 'test'])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.array(task['train'][0]['input']).shape\n#img = task['train'][0]\n#x = img['input']\n#x = np.array(x)\n#z = np.array([11]*15).reshape(15,1)\n#f = np.concatenate((x,z),axis = 1).flatten()\n#f = np.insert(f,0,10)\n#f[-1] = 12\n#np.random.choice(4)\n#np.random.choice(5,4,replace = False)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_task(task):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 4, figsize=(15,15))\n    axs[0].imshow(task['train'][0]['input'], cmap=cmap, norm=norm)\n    axs[0].axis('off')\n    axs[0].set_title('Train Input')\n    axs[1].imshow(task['train'][0]['output'], cmap=cmap, norm=norm)\n    axs[1].axis('off')\n    axs[1].set_title('Train Output')\n    axs[2].imshow(task['test'][0]['input'], cmap=cmap, norm=norm)\n    axs[2].axis('off')\n    axs[2].set_title('Test Input')\n    axs[3].imshow(task['test'][0]['output'], cmap=cmap, norm=norm)\n    axs[3].axis('off')\n    axs[3].set_title('Test Output')\n    plt.tight_layout()\n    plt.show()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_task(task)","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x1080 with 4 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rotate(a,b):\n    i = np.random.choice(4)\n    i = i*90\n    a = ndimage.rotate(a,i)\n    b = ndimage.rotate(b,i)\n    return(a,b)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#color swap fixes black and gray.\n\ndef colour_swap(a,b):\n    l = [1,2,3,4,6,7,8,9]\n    t = np.random.choice(l,8,replace = False)\n    d = {}\n    for j in range(8):\n        d[l[j]] = t[j]\n    d[0] = 0\n    d[5] = 5\n    x = a.flatten()\n    y = b.flatten()\n    a1 = np.array([d[z] for z in x])\n    b1 = np.array([d[z] for z in y])\n    a1 = a1.reshape(a.shape)\n    b1 = b1.reshape(b.shape)\n    return(a1,b1)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_sequence(a,target):\n    r,c = a.shape\n    z = np.array([11]*r).reshape(r,1)\n    f = np.concatenate((a,z),axis = 1).flatten()\n    if target:\n        f = np.insert(f,0,10)\n        f = np.append(f,12)\n    return(f)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_np(a,b):\n    a = np.array(a)\n    b = np.array(b)\n    return(a,b)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dims(task):\n    i = []\n    o = []\n    for d in task['train']:\n        r,c = np.array(d['input']).shape\n        i.append((r+1)*c)\n        r,c = np.array(d['output']).shape\n        o.append((r+1)*c)\n    i1 = max(i)\n    o1 = max(o)\n    return(i1,o1+2)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# colours go from 1 ---> 10, \n# End of lines are denoted by 12\n# start of the whole sequence (sos token) is 11\n# End of sequence (eos) token is 13\n\ndef augment(task,sam_len):\n    m = len(task['train'])\n    l1 = []\n    l2 = []\n    in_d,o_d = get_dims(task)\n    inputs_e = np.full((sam_len,in_d),-1)\n    inputs_d = np.full((sam_len,1),-1)\n    targets = np.full((sam_len,o_d),-1)\n    for j in range(sam_len):\n        i = np.random.choice(m)\n        #i = 0\n        a = task['train'][i]['input']\n        b = task['train'][i]['output']\n        a,b = make_np(a,b)\n        a,b = rotate(a,b)\n        a,b = colour_swap(a,b)\n        a = make_sequence(a,False) #target = False\n        b = make_sequence(b,True)  #target = True\n        inputs_e[j,:len(a)] = a\n        inputs_d[j,0] = 10\n        targets[j,:len(b)-1] = b[1:]\n    return(inputs_e+1,inputs_d+1,targets+1)    ","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_batch(inputs_e,inputs_d,targets,task,sam_len,batch_size):\n  # inputs_e,inputs_d,targets = augment(task,sam_len)\n    while True:\n          for j in range(0,len(inputs_e),batch_size):\n                a = inputs_e[j:j+batch_size]\n                b = inputs_d[j:j+batch_size]\n                c = targets[j:j+batch_size]\n                q = len(c[0])\n                t_new = np.zeros((batch_size,q,14))\n                for i in range(batch_size):\n                    for k in range(q):\n                        if c[i,k] > 0:\n                            t_new[i,k,c[i,k]] = 1\n                        else:\n                            t_new[i,k,c[i,k]] = 0\n                yield(a,t_new)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_samples = len(X_train)\n#val_samples = len(X_test)\nbatch_size = 32\nepochs = 10\nlatent_dim = 256","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_inputs = Input(shape=(None,))\nenc_emb =  Embedding(14,latent_dim,mask_zero = True)(encoder_inputs)\nencoder_lstm = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n# We discard `encoder_outputs` and only keep the states.\nstates = [state_h, state_c]","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(1,))\ndec_emb_layer = Embedding(14,latent_dim, mask_zero = True)\n#dec_emb = dec_emb_layer(decoder_inputs)\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n#decoder_outputs, _, _ = decoder_lstm(dec_emb,initial_state=encoder_states)\ndecoder_dense = Dense(14, activation='softmax')\n#decoder_outputs = decoder_dense(decoder_outputs)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_decoder_seq_length = 100","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_outputs = []\ninputs = decoder_inputs\nfor _ in range(max_decoder_seq_length):\n    x,state_h,state_c = decoder_lstm(inputs,initial_state = states)\n    outputs = decoder_dense(x)\n    all_outputs.append(outputs)\n    inputs = outputs\n    states = [state_h,state_c]","execution_count":23,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Input 0 is incompatible with layer lstm_2: expected ndim=3, found ndim=2","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-a94136626074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_decoder_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mall_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'constants'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'constants'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_2: expected ndim=3, found ndim=2"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)","execution_count":24,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Lambda' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-b7c3c4d1a775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'Lambda' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model([encoder_inputs, decoder_inputs], decoder_outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer= 'rmsprop', loss= 'categorical_crossentropy', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nsam_len = 512\nepochs = 15\nval_len = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs_e,inputs_d,targets = augment(task,sam_len)\ninputs_1e,inputs_1d,targets1 = augment(task,sam_len)\nmodel.fit_generator(generator = generate_batch(inputs_e,inputs_d,targets,task,sam_len,batch_size),\n                steps_per_epoch = sam_len//batch_size,\n                epochs = epochs,\n                validation_data = generate_batch(inputs_1e,inputs_1d,targets1,task,val_len,batch_size),\n                validation_steps = val_len//batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode the input sequence to get the \"Context vectors\"\nencoder_model = Model(encoder_inputs, encoder_states)\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs)\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_input)\ndecoder_states2 = [state_h2, state_c2]\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_outputs2)\n# Final decoder model\ndecoder_model = Model([decoder_inputs] + decoder_state_input,[decoder_outputs2] + decoder_states2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    # Populate the first character of \n    #target sequence with the start character.\n    target_seq[0, 0] = 11\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sequence = []\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n        # Sample a token\n        sampled_index= np.argmax(output_tokens[0, -1, :])\n        decoded_sequence.append(sampled_index)\n        # Exit condition: either hit max length\n        # or find stop character.\n        if len(decoded_sequence) > 300:\n        #(sampled_index == 13 or\n            stop_condition = True\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_index\n        # Update states\n        states_value = [h, c]\n    return decoded_sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_seq = np.array(task['train'][0]['input'])\ninput_seq = make_sequence(input_seq,False)+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_seq = np.array(task['train'][0]['output'])\nout_seq = make_sequence(out_seq,False)+1\nout_seq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decode_sequence(input_seq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs_e,inputs_d,targets = augment(task,sam_len)\ninputs_1e,inputs_1d,targets1 = augment(task,sam_len)\nmodel12.fit_generator(generator = generate_batch(inputs_e,inputs_d,targets,task,sam_len,batch_size),\n                steps_per_epoch = sam_len//batch_size,\n                epochs = epochs,\n                validation_data = generate_batch(inputs_1e,inputs_1d,targets1,task,val_len,batch_size),\n                validation_steps = val_len//batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}